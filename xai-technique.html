<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>XAI Explained</title>
    <link rel="stylesheet" href="base.css">
    <link rel="stylesheet" href="xai-technique.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <!--navbar-->
    <nav class="navbar">
      <div class="logo">XAI Explained</div>
      <ul class="nav-links">
          <li><a href="what-is-xai.html">What is XAI?</a></li>
          <li><a href="xai-key-concept.html">Key Concepts</a></li>
          <li><a href="xai-technique.html">XAI Techniques</a></li>
          <li><a href="application.html">Applications</a></li>
          <li><a href="challenges.html">Challenges</a></li>
          <li><a href="future.html">Future of XAI</a></li>
          <li><a href="resource.html">Resources</a></li>
      </ul>
  </nav>

  <!--Content-->
  <div class="technique-intro">
    <h1>XAI Techniques</h1>
    <img src="/images/technique.png" alt="XAI Techniques Overview" class="technique-hero">
    <p>
        Explainable AI (XAI) provides transparency into AI decisions using various methods. These techniques help make complex models interpretable, 
        increasing trust and usability in real-world applications. Below we explore the most important XAI methods categorized by their approach.
    </p>
</div>


<div class="technique-categories">
    <!-- feature attribution -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-chart-line"></i>
        </div>
        <div class="card-content">
            <h2>Feature Attribution Methods</h2>
            <p class="card-description">
                These techniques identify which features influence a model's decision the most by assigning importance scores.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>SHAP (SHapley Additive exPlanations):</b> Game theory-based approach that fairly distributes contribution scores to input features.</li>
                    <li><b>LIME (Local Interpretable Model-agnostic Explanations):</b> Builds simple interpretable models to approximate complex AI decisions locally.</li>
                    <li><b>Integrated Gradients:</b> Attributes importance by integrating the model's gradients along a path from baseline to input.</li>
                    <li><b>Feature Importance:</b> Highlights which features impact predictions through permutation or impurity-based methods.</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In healthcare diagnostics, SHAP values can show that high cholesterol (contribution: 35%) and elevated blood pressure (contribution: 28%) were the primary factors in an AI-based heart disease prediction model.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Example-Based Explanations -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-search"></i>
        </div>
        <div class="card-content">
            <h2>Example-Based Explanations</h2>
            <p class="card-description">
                AI decisions are explained by comparing them to past similar cases or prototypes.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>Case-Based Reasoning:</b> Uses previous similar cases to justify new decisions.</li>
                    <li><b>K-Nearest Neighbors (KNN):</b> Predicts based on the most similar past examples in the feature space.</li>
                    <li><b>Prototype Selection:</b> Identifies representative examples that capture model behavior.</li>
                    <li><b>Counterfactual Explanations:</b> Shows what minimal changes would lead to a different outcome.</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In legal AI systems, case-based reasoning helps predict case outcomes by referencing 5 most similar past court rulings with 82% similarity score, while counterfactual explanations might show that reducing the defendant's prior convictions by 1 would change the predicted sentence length from 3-5 years to 1-3 years.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- surrogate_model -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-project-diagram"></i>
        </div>
        <div class="card-content">
            <h2>Surrogate Models</h2>
            <p class="card-description">
                Simpler interpretable models approximate complex AI to provide global explanations.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>Decision Tree Surrogates:</b> Uses decision trees to mimic black-box model behavior.</li>
                    <li><b>Rule-Based Surrogates:</b> Extracts human-readable rules that approximate model logic.</li>
                    <li><b>Linear Regression Surrogates:</b> Provides simplified linear interpretations of complex models.</li>
                    <li><b>Anchors:</b> High-precision rules that "anchor" predictions with sufficient conditions.</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In credit scoring, a surrogate decision tree with 5 levels can explain that applications are primarily denied when: (1) credit score < 580 (weight: 45%), (2) debt-to-income ratio > 43% (weight: 30%), and (3) recent bankruptcies > 0 (weight: 25%).</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Visualization-Based Techniques -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-eye"></i>
        </div>
        <div class="card-content">
            <h2>Visualization-Based Techniques</h2>
            <p class="card-description">
                Graphical methods illustrate how AI models process inputs and make decisions.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>Saliency Maps:</b> Highlights key areas in an image affecting predictions.</li>
                    <li><b>Activation Maps:</b> Shows how deep learning layers respond to inputs.</li>
                    <li><b>Attention Mechanisms:</b> Visualizes which parts of input the model focuses on.</li>
                    <li><b>Dimensionality Reduction:</b> Projects high-dimensional data for human interpretation (t-SNE, UMAP).</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In medical imaging analysis, AI-generated heatmaps highlight suspicious regions in chest X-rays with 92% confidence, showing radiologists exactly where potential abnormalities (like tumors) are detected, while attention maps reveal the model focuses 70% of its analysis on the upper right lung quadrant.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- New Section: Model-Specific Techniques -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-cogs"></i>
        </div>
        <div class="card-content">
            <h2>Model-Specific Techniques</h2>
            <p class="card-description">
                Explanation methods designed specifically for certain model architectures.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>TreeSHAP:</b> Efficient SHAP implementation for tree-based models (Random Forests, XGBoost).</li>
                    <li><b>CNN Filters Visualization:</b> Shows what patterns convolutional neural networks detect.</li>
                    <li><b>Transformer Attention:</b> Visualizes attention heads in transformer models.</li>
                    <li><b>GAN Latent Space:</b> Explores how GANs generate outputs through latent space.</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In natural language processing, visualizing BERT's attention heads reveals that for sentiment analysis, the model primarily focuses on adjectives (35% attention) and negation words (25% attention), while largely ignoring articles and prepositions (<5% attention each).</p>
                </div>
            </div>
        </div>
    </div>

    <!-- New Section: Rule Extraction -->
    <div class="technique-card">
        <div class="card-icon">
            <i class="fas fa-code-branch"></i>
        </div>
        <div class="card-content">
            <h2>Rule Extraction</h2>
            <p class="card-description">
                Techniques that extract human-understandable rules from complex models.
            </p>
            <div class="technique-details">
                <h3>Key Methods:</h3>
                <ul>
                    <li><b>Decision Rules:</b> Extracts IF-THEN rules from model behavior.</li>
                    <li><b>Skope-Rules:</b> Finds high-performance, interpretable rules.</li>
                    <li><b>Bayesian Rule Lists:</b> Ordered if-then rules with probabilistic interpretations.</li>
                    <li><b>Fuzzy Rules:</b> Handles uncertainty with degrees of truth.</li>
                </ul>
                <div class="example-section">
                    <h3>Real-World Application:</h3>
                    <p>In predictive maintenance, extracted rules might show: "IF (vibration > 4.2mm/s) AND (temperature > 85¬∞C) AND (runtime > 800 hours) THEN (failure_probability > 78%) with 94% precision." Such rules can be directly implemented in monitoring dashboards.</p>
                </div>
            </div>
        </div>
    </div>
</div>

<!-- Technique Comparison Table -->
<div class="comparison-section">
    <h2>XAI Technique Comparison</h2>
    <table class="technique-comparison">
        <thead>
            <tr>
                <th>Technique</th>
                <th>Model Type</th>
                <th>Scope</th>
                <th>Complexity</th>
                <th>Best For</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>SHAP/LIME</td>
                <td>Model-agnostic</td>
                <td>Local</td>
                <td>Medium</td>
                <td>Feature importance</td>
            </tr>
            <tr>
                <td>Saliency Maps</td>
                <td>Neural Networks</td>
                <td>Local</td>
                <td>Low</td>
                <td>Computer vision</td>
            </tr>
            <tr>
                <td>Decision Rules</td>
                <td>Model-agnostic</td>
                <td>Global</td>
                <td>Low</td>
                <td>Business rules</td>
            </tr>
            <tr>
                <td>Counterfactuals</td>
                <td>Model-agnostic</td>
                <td>Local</td>
                <td>High</td>
                <td>Actionable insights</td>
            </tr>
            <tr>
                <td>Attention</td>
                <td>Transformers</td>
                <td>Both</td>
                <td>Medium</td>
                <td>NLP tasks</td>
            </tr>
        </tbody>
    </table>
</div>

<!-- page nav -->
<div class="page-navigation">
  <a href="xai-key-concept.html" class="prev">&larr; Previous: Key Concepts</a>
  <a href="application.html" class="next">Next: Applications &rarr;</a>
</div>

<!--footer -->
<footer>
    <div class="footer-container">
      <div class="footer-section">
        <h3>üì© Contact Me</h3>
        <p>Email: <a href="mailto:priyanshissolanki7@gmail.com" id="email">priyanshissolanki7@gmail.com</a></p>
        <p>LinkedIn: <a href="https://www.linkedin.com/in/priyanshisolanki/" id="linkedin" target="_blank">Priyanshi Solanki</a></p>
      </div>

      <div class="footer-section">
        <h3>üìù Feedback</h3>
        <p>Have suggestions? Share your thoughts!</p>
        <a href="website/feedback.html" target="_blank" class="btn">Give Feedback</a>
      </div>

      <div class="footer-section">
        <h3>ü§ù Contribute</h3>
        <p>Want to improve this project? Check out the repo!</p>
        <a href="https://github.com/Priyanshi-Solanki/xai_explained.git" target="_blank" class="btn">Contribute on GitHub</a>
      </div>

      <div class="footer-section">
        <h3>üí¨ Join the Discussion</h3>
        <p>Discuss XAI with the community!</p>
        <a href="https://discord.gg/yourserver" target="_blank" class="btn">Join Discord</a>
      </div>
    </div>

    <div class="footer-bottom">
      <p>¬© 2025 XAI Explained | Built by Priyanshi Solanki</p>
    </div>
</footer>

</body>

</html>


